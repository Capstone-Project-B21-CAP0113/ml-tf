{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963",
   "display_name": "Python 3.9.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Text Classification Notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install tensorflow\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install PySastrawi\n",
    "!pip install sklearn\n",
    "!pip install keras-tuner"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning related imports\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and generic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "import string\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "filepath = os.getcwd()\n",
    "datasetpath =  os.path.join(filepath, \"cleaned_data\", \"laporanencoded.csv\")\n",
    "\n",
    "# Github URL for dataset when used in Google Colab\n",
    "github_url = \"https://raw.githubusercontent.com/Capstone-Project-B21-CAP0113/ml-tf/main/laporan/cleaned_data/laporanencoded.csv\"\n",
    "\n",
    "laporan = pd.read_csv(datasetpath, encoding=\"ISO-8859-1\")\n",
    "# Print dataset shape\n",
    "print(laporan.shape)\n",
    "# Print dataset head\n",
    "laporan.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text and labels\n",
    "label_list = [\n",
    "    \"perselisihan\",\n",
    "    \"infrastruktur\",\n",
    "    \"pemerintah\",\n",
    "    \"kesehatan\",\n",
    "    \"teknologi\",\n",
    "    \"administrasi\",\n",
    "    \"fasilitas\",\n",
    "    \"lingkungan\",\n",
    "    \"ketertiban\",\n",
    "    \"listrik\",\n",
    "    \"bahaya\",\n",
    "    \"lainnya\",\n",
    "    \"pungli\",\n",
    "    \"ilegal\",\n",
    "    \"lalulintas\",\n",
    "    \"bencana\",\n",
    "    \"air\",\n",
    "    \"pendidikan\",\n",
    "    \"kebersihan\",\n",
    "    \"sosial\",\n",
    "    \"wisata\",\n",
    "    \"sara\",\n",
    "    \"pencurian\",\n",
    "    \"korupsi\",\n",
    "    \"bbm\",\n",
    "    \"keuangan\"\n",
    "] \n",
    "x = laporan[\"text\"]\n",
    "y = laporan[label_list]\n",
    "\n",
    "# Print number of element in each category ( one element can have many label since it's a multi label classification )\n",
    "for i in label_list:\n",
    "    print(\"{}: {}\".format(i, (laporan[i] == 1).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text head\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label head\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and stem words using Sastrawi\n",
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stopword = stopword_factory.create_stop_word_remover()\n",
    "\n",
    "for i in range(len(x)):\n",
    "    x[i] = stopword.remove(x[i])\n",
    "    x[i] = stemmer.stem(x[i])\n",
    "\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad text\n",
    "vocab_size = 2000\n",
    "embedding_dim = 16\n",
    "max_length = 300\n",
    "trunc_type = \"post\"\n",
    "oov_token = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(x)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(x)\n",
    "\n",
    "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shuffle, batch and separate data into train, dev and test\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "DATASET_SIZE = len(x)\n",
    "\n",
    "\n",
    "tx = tf.convert_to_tensor(padded)\n",
    "ty = tf.convert_to_tensor(y)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((tx, ty))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "train_set = dataset.take(int(0.8 * len(dataset)))\n",
    "test_set = dataset.skip(int(0.8 * len(dataset)))\n",
    "test_set = dataset.take(int(0.2 * len(dataset))) \n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 128),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(26, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 75\n",
    "history = model.fit(train_set, epochs=NUM_EPOCHS, validation_data=test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function for inferrence\n",
    "def treatinput(inp):\n",
    "    # Make string into lowercase string\n",
    "    treated = inp.lower()\n",
    "    # remove punctuation\n",
    "    treated = treated.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    # remove trailing whitespace\n",
    "    treated = treated.strip()\n",
    "    # Remove stopwords\n",
    "    treated = stopword.remove(treated)\n",
    "    # Stem string\n",
    "    treated = stemmer.stem(treated)\n",
    "    return treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample inferrece\n",
    "sample_laporan = \"Jalan sukamaju jaya macet sudah dari pagi, lampu merah mati dan tidak ada polisi lalu lintas\"\n",
    "treated_input = treatinput(sample_laporan)\n",
    "print(treated_input)\n",
    "pad = pad_sequences(tokenizer.texts_to_sequences([treated_input]), maxlen=max_length, truncating=trunc_type)\n",
    "prediction = model.predict(pad)\n",
    "\n",
    "res = dict(zip(label_list, prediction[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print predictions\n",
    "for i in res.keys():\n",
    "    # if(res[i] > 0.6):\n",
    "    print(\"{} {:.5f}\".format(i, res[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a new model with Keras Tuner // CAREFUL THIS TAKES A LONG TIME\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "def model_builder(hp):\n",
    "    emb_units = hp.Int('emb_units', min_value=16, max_value=256, step=16)\n",
    "    bid1_units = hp.Int('bid1_units', min_value=16, max_value=256, step=16)\n",
    "    drop1_frac = hp.Float('drop1_frac', min_value=0.1, max_value=0.8, step=0.1)\n",
    "    bid2_units = hp.Int('bid2_units', min_value=16, max_value=256, step=16)\n",
    "    drop2_frac = hp.Float('drop2_frac', min_value=0.1, max_value=0.8, step=0.1)\n",
    "    dense1_units = hp.Int('dense1_unit', min_value=16, max_value=256, step=16)\n",
    "    drop3_frac = hp.Float('drop3_frac', min_value=0.1, max_value=0.8, step=0.1)\n",
    "    dense2_units = hp.Int('dense2_unit', min_value=16, max_value=256, step=16)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, emb_units),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(bid1_units, return_sequences=True)),\n",
    "    tf.keras.layers.Dropout(drop1_frac),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(bid2_units)),\n",
    "    tf.keras.layers.Dropout(drop2_frac),\n",
    "    tf.keras.layers.Dense(dense1_units, activation='relu'),\n",
    "    tf.keras.layers.Dropout(drop3_frac),\n",
    "    tf.keras.layers.Dense(dense2_units, activation='relu'),\n",
    "    tf.keras.layers.Dense(26, activation='sigmoid')\n",
    "])\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder, objective='val_accuracy', max_epochs=100, factor=3, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_set, epochs=100, validation_data=test_set, callbacks=[stop_early])\n",
    "\n",
    "best_hps= tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hps)"
   ]
  },
  {
   "source": [
    "# Trial 5 Complete [00h 03m 30s]\n",
    "# val_accuracy: 0.2395833283662796\n",
    "\n",
    "# Best val_accuracy So Far: 0.2534722089767456\n",
    "# Total elapsed time: 00h 55m 20s\n",
    "\n",
    "# Search: Running Trial #6\n",
    "\n",
    "# Hyperparameter    |Value             |Best Value So Far \n",
    "# emb_units         |48                |32                \n",
    "# bid1_units        |240               |112               \n",
    "# drop1_frac        |0.6               |0.3               \n",
    "# bid2_units        |176               |112               \n",
    "# drop2_frac        |0.4               |0.4               \n",
    "# dense1_unit       |176               |112               \n",
    "# drop3_frac        |0.7               |0.4               \n",
    "# dense2_unit       |144               |224               \n",
    "# learning_rate     |1e-05             |0.01              \n",
    "# tuner/epochs      |2                 |2                 \n",
    "# tuner/initial_e...|0                 |0                 \n",
    "# tuner/bracket     |2                 |2                 \n",
    "# tuner/round       |0                 |0                 \n",
    "\n",
    "# Epoch 1/2\n",
    "# 72/72 [==============================] - 618s 8s/step - loss: 0.6918 - accuracy: 0.0573 - val_loss: 0.6889 - val_accuracy: 0.0625\n",
    "# Epoch 2/2\n",
    "# 72/72 [==============================] - 623s 9s/step - loss: 0.6846 - accuracy: 0.0760 - val_loss: 0.6749 - val_accuracy: 0.0521"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = tuner.hypermodel.build(best_hps)\n",
    "history = model_t.fit(dataset, epochs=100, validation_data=test_set)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "hypermodel.fit(dataset, epochs=best_epoch, validation_data=test_set)"
   ]
  }
 ]
}